== Example Flow 2

In this second example, the **flow** takes a `globus-transfer-transfer#0.10` object specifying a list of source files that exist on a user-provided source **collection**, distinct from the **collection** which is colocated with the Compute **endpoint**.

The **flow** transfers the files to the **collection** which is colocated with the Compute **endpoint**, creates a tarfile from them, and transfers the tarfile to a user-provided destination **collection**. This flow behaves the same as its counterpart, the only difference being in that the source files to be tarballed and the desitnation collection are derived from the transfer data rather than the user input. This **flow** will fail if any source paths are the path '/' or contain the character '~'.

The **flow** is intended to be invoked from the Globus webapp when initiating a transfer where the **destination** collection has this flow set as the destination in its `associated_flow_policy`.

The **flow** will:

1. Set constants for the **run**
2. Validate the input source paths
3. Collect the destination paths for the input files on the intermediate **collection**
4. Create an output directory named after the **run**'s ID on the intermediate **collection**
5. Transfer the source files to the newly created output directory folder on the intermediate **collection**
6. Invoke the `do_tar` **function** to create a tar archive from the source files and save it in the output directory
7. Transfer the resulting tarfile to the destination **collection** provided in the **flow** input
8. Delete the output directory on the intermediate **collection**


=== Create the **Flow**

1. Edit `compute_transfer_example_2_definition.json` and replace the placeholder values:

   - `gcs_endpoint_id`: The intermediate **collection** ID
   - `compute_endpoint_id`: The Compute **endpoint** ID
   - `compute_function_id`: The UUID of the registered `do_tar` **function**

2. Create the flow:
+
[source,bash,role=clippable-code]
----
globus flows create "Compute and Transfer Flow Example 2" \
    ./compute_transfer_example_2_definition.json \
    --input-schema ./compute_transfer_example_2_schema.json
----
+
Or update the existing **flow** from example 1:
+
[source,bash,role=clippable-code]
----
globus flows update <FLOW_ID> \
    --title "Compute and Transfer Flow Example 2" \
    --definition ./compute_transfer_example_2_definition.json \
    --input-schema ./compute_transfer_example_2_schema.json
----

3. Save the **flow** ID returned by this command

ifndef::env-github[]
[.accordionize]
--
.compute_transfer_example_2_definition.json
[%collapsible]
====
[source,json,role=clippable-code]
----
include::compute_transfer_example_2_definition.json[]
----
====
.compute_transfer_example_2_schema.json
[%collapsible]
====
[source,json,role=clippable-code]
----
include::compute_transfer_example_2_schema.json[]
----
====
--
endif::[]

=== Run the **Flow**

1. Create the **flow** input JSON file:
+
[source,json,role=clippable-code]
----
{
    "destination_path": "/path/to/your/destination/file.tar.gz",
}
----
+
[NOTE]
======
`destination_path` is an optional property. If left unset, the flow will try to save the output tarball to `/~/<random UUID>.tar.gz`.
======

2. Start the **flow**:
+
[source,bash,role=clippable-code]
----
globus flows start "$FLOW_ID" \
    --input <YOUR FLOW INPUT FILE> \
    --label "Compute and Transfer Flow Example 2 Run"
----
+
And save the **run** ID for use in the next command.

3. Monitor the **run** progress:
+
[source,bash,role=clippable-code]
----
globus flows run show "<RUN_ID>"
----
** At this point, the **run** _may_ become `INACTIVE`, depending on the type of **collection** being used.
** For inactive **run**s due to data access requirements, this can be resolved by resuming the **run** and following the prompts:
+
[source,bash,role=clippable-code]
----
globus flows run resume "<RUN_ID>"
----
+
When prompted, run `globus session consent` and rerun `globus flows run resume` to resume the **run**.